{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Bert 的Pytorch实现"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 准备数据集"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "# import依赖包\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import re    # 对数据集进行分句子，以及删除不需要的标点符号\n",
                "import math\n",
                "from random import random  # 生辰随机数\n",
                "from random import randrange\n",
                "from random import shuffle\n",
                "from random import randint\n",
                "import torch.optim as optim\n",
                "import torch.utils.data as Data"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# 构造fake trian data\n",
                "text = (\n",
                "    'Hello, how are you? I am Romeo.\\n' # R\n",
                "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
                "    'Nice meet you too. How are you today?\\n' # R\n",
                "    'Great. My baseball team won the competition.\\n' # J\n",
                "    'Oh Congratulations, Juliet\\n' # R\n",
                "    'Thank you Romeo\\n' # J\n",
                "    'Where are you going today?\\n' # R\n",
                "    'I am going shopping. What about you?\\n' # J\n",
                "    'I am going to visit my grandmother. she is not very well' # R\n",
                ")\n",
                "\n",
                "# 对数据集进行分句子，以及删除不需要的标点符号\n",
                "sentences = re.sub(\"[,.!?\\\\-]\", \"\", text.lower()).split(\"\\n\")\n",
                "\n",
                "## 注意实际上text的处理会更加繁琐，而且真实的input text也不会简单可以按照\\n 来进行分句处理。\n",
                "# 不重复的word vocab\n",
                "vocab = list(set(\" \".join(sentences).split()))\n",
                "word2idx = {'[PAD]':0, '[CLS]':1, '[SEP]':2, '[MASK]':3}\n",
                "for i, w in enumerate(vocab):\n",
                "    word2idx[w] = i + 4\n",
                "vocab_size = len(word2idx)\n",
                "\n",
                "token_list = []\n",
                "for sentence in sentences:\n",
                "    temp = [word2idx[s] for s in sentence.split()]\n",
                "    token_list.append(temp)\n",
                "token_list"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[[12, 28, 27, 20, 33, 5, 30],\n",
                            " [12, 30, 11, 39, 9, 4, 29, 18, 23, 20],\n",
                            " [29, 23, 20, 38, 28, 27, 20, 34],\n",
                            " [8, 11, 10, 35, 15, 16, 22],\n",
                            " [19, 36, 4],\n",
                            " [6, 20, 30],\n",
                            " [21, 27, 20, 17, 34],\n",
                            " [33, 5, 17, 25, 13, 24, 20],\n",
                            " [33, 5, 17, 18, 14, 11, 26, 37, 9, 7, 32, 31]]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# 模型的config文件参数\n",
                "# BERT parameter\n",
                "max_seq_len = 30\n",
                "batch_size = 6\n",
                "max_pred = 5\n",
                "layers_num = 6\n",
                "heads_num = 12\n",
                "model_dim = 756\n",
                "ffn_dim = 756 * 4\n",
                "per_head_dim = 756 / 12\n",
                "segments_num = 2"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 数据预处理\n",
                "* 按照MASK的标准深沉数据： 一句话中的15%被MASK，被MASK中 80%被[MASK] 替换，10% 被随机替换，10% 不做任何变化\n",
                "* 构造Dataloader，方便数据训练的时候进行迭代"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "# make fake data\n",
                "def make_data():\n",
                "    \"\"\"构造一个batch_size（6） 的样本，其中NSP任务为positive和negative的样本各3个 \n",
                "    \"\"\"\n",
                "    batch = []\n",
                "    positive = negative = 0\n",
                "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
                "        # 一条数据一条数据的生成\n",
                "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
                "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
                "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
                "        segment_ids = [0] + len(tokens_a) * [0] + [0] + [1] * len(tokens_b) + [1]\n",
                "        # MASK LM\n",
                "        ## 按照 sentence 长度的15%来确定需要 MASK 的位置\n",
                "        n_pred = min(max_pred, max(int(len(input_ids) * 0.15), 1))\n",
                "        # 过滤special token得到备选mask的index\n",
                "        cand_mask_pos = [i for i, v in enumerate(input_ids) if v > 3]\n",
                "        shuffle(cand_mask_pos)\n",
                "        masked_pos, masked_token = [], []\n",
                "        for pos in cand_mask_pos[:n_pred]:\n",
                "            masked_pos.append(pos)\n",
                "            masked_token.append(input_ids[pos])\n",
                "            random_value = random()\n",
                "            if random_value < 0.8:\n",
                "                input_ids[pos] = word2idx['[MASK]']\n",
                "            elif random_value > 0.9:\n",
                "                # 随机选择其他token 进行替换\n",
                "                index = randint(0, vocab_size -1)\n",
                "                while index < 3 or index == input_ids[pos]:\n",
                "                    index = randint(0, vocab_size -1)\n",
                "                input_ids[pos] = index\n",
                "            \n",
                "        # zero padding 的token 也要mask 15% （这一步很奇怪）\n",
                "        if max_pred > n_pred:\n",
                "            n_pad = max_pred - n_pred\n",
                "            masked_token = masked_token + [0] * n_pad\n",
                "            masked_pos = masked_pos + [0] * n_pad\n",
                "\n",
                "        # zero-padding\n",
                "        n_pads = max_seq_len - len(input_ids)\n",
                "        input_ids = input_ids + n_pads * [0]\n",
                "        segment_ids = segment_ids + n_pads * [0]\n",
                "        \n",
                "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
                "            batch.append([input_ids, segment_ids, masked_token, masked_pos, True])\n",
                "            positive += 1\n",
                "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
                "            batch.append([input_ids, segment_ids, masked_token, masked_pos, False])\n",
                "            negative += 1\n",
                "    return batch\n",
                "\n",
                "batch = make_data()\n",
                "print(len(batch))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "6\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "input_ids, segment_ids, masked_token, masked_pos, isNext = zip(*batch)\n",
                "\n",
                "class MyDataSet(Data.Dataset):\n",
                "\n",
                "    def __init__(self, input_ids, segment_ids, masked_token, maksed_pos, isNext):\n",
                "        self.input_ids = input_ids\n",
                "        self.segment_ids = segment_ids\n",
                "        self.masked_token = masked_token\n",
                "        self.masked_pos = maksed_pos\n",
                "        self.isNext = isNext\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.input_ids)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.input_ids[idx], self.segment_ids[idx], self.masked_token[idx], self.masked_pos[idx], self.isNext[idx]\n",
                "\n",
                "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_token, masked_pos, isNext))"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "56e331e9a2ee1d59a709fc58c9503a214b87f78fd6d7d2d4ac9f44a5699c674f"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}