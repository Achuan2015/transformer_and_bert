{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Bert 的Pytorch实现\n",
                "\n",
                "```\n",
                "内容参考blog：https://wmathor.com/index.php/archives/1457/\n",
                "```"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 准备数据集"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "# import依赖包\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import re    # 对数据集进行分句子，以及删除不需要的标点符号\n",
                "import math\n",
                "from random import random  # 生辰随机数\n",
                "from random import randrange\n",
                "from random import shuffle\n",
                "from random import randint\n",
                "import torch.optim as optim\n",
                "import torch.utils.data as Data"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "# 构造fake trian data\n",
                "text = (\n",
                "    'Hello, how are you? I am Romeo.\\n' # R\n",
                "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
                "    'Nice meet you too. How are you today?\\n' # R\n",
                "    'Great. My baseball team won the competition.\\n' # J\n",
                "    'Oh Congratulations, Juliet\\n' # R\n",
                "    'Thank you Romeo\\n' # J\n",
                "    'Where are you going today?\\n' # R\n",
                "    'I am going shopping. What about you?\\n' # J\n",
                "    'I am going to visit my grandmother. she is not very well' # R\n",
                ")\n",
                "\n",
                "# 对数据集进行分句子，以及删除不需要的标点符号\n",
                "sentences = re.sub(\"[,.!?\\\\-]\", \"\", text.lower()).split(\"\\n\")\n",
                "\n",
                "## 注意实际上text的处理会更加繁琐，而且真实的input text也不会简单可以按照\\n 来进行分句处理。\n",
                "# 不重复的word vocab\n",
                "vocab = list(set(\" \".join(sentences).split()))\n",
                "word2idx = {'[PAD]':0, '[CLS]':1, '[SEP]':2, '[MASK]':3}\n",
                "for i, w in enumerate(vocab):\n",
                "    word2idx[w] = i + 4\n",
                "idx2word = {i: w for w, i in word2idx.items()}\n",
                "vocab_size = len(word2idx)\n",
                "\n",
                "token_list = []\n",
                "for sentence in sentences:\n",
                "    temp = [word2idx[s] for s in sentence.split()]\n",
                "    token_list.append(temp)\n",
                "token_list"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[[30, 10, 6, 22, 32, 21, 39],\n",
                            " [30, 39, 7, 19, 28, 15, 8, 37, 24, 22],\n",
                            " [8, 24, 22, 29, 10, 6, 22, 33],\n",
                            " [25, 7, 4, 16, 13, 9, 17],\n",
                            " [5, 26, 15],\n",
                            " [14, 22, 39],\n",
                            " [36, 6, 22, 12, 33],\n",
                            " [32, 21, 12, 23, 27, 31, 22],\n",
                            " [32, 21, 12, 37, 38, 7, 11, 20, 28, 34, 35, 18]]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 10
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# 模型的config文件参数\n",
                "# BERT parameter\n",
                "max_seq_len = 30\n",
                "batch_size = 6\n",
                "max_pred = 5\n",
                "layers_num = 6\n",
                "heads_num = 12\n",
                "model_dim = 756\n",
                "ffn_dim = 756 * 4\n",
                "per_head_dim = 756 / 12\n",
                "segments_num = 2\n",
                "epoch_num = 180\n",
                "dropout=0.1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 数据预处理\n",
                "* 按照MASK的标准深沉数据： 一句话中的15%被MASK，被MASK中 80%被[MASK] 替换，10% 被随机替换，10% 不做任何变化\n",
                "* 构造Dataloader，方便数据训练的时候进行迭代"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# make fake data\n",
                "def make_data():\n",
                "    \"\"\"构造一个batch_size（6） 的样本，其中NSP任务为positive和negative的样本各3个 \n",
                "    \"\"\"\n",
                "    batch = []\n",
                "    positive = negative = 0\n",
                "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
                "        # 一条数据一条数据的生成\n",
                "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
                "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
                "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
                "        segment_ids = [0] + len(tokens_a) * [0] + [0] + [1] * len(tokens_b) + [1]\n",
                "        # MASK LM\n",
                "        ## 按照 sentence 长度的15%来确定需要 MASK 的位置\n",
                "        n_pred = min(max_pred, max(int(len(input_ids) * 0.15), 1))\n",
                "        # 过滤special token得到备选mask的index\n",
                "        cand_mask_pos = [i for i, v in enumerate(input_ids) if v > 3]\n",
                "        shuffle(cand_mask_pos)\n",
                "        masked_pos, masked_token = [], []\n",
                "        for pos in cand_mask_pos[:n_pred]:\n",
                "            masked_pos.append(pos)\n",
                "            masked_token.append(input_ids[pos])\n",
                "            random_value = random()\n",
                "            if random_value < 0.8:\n",
                "                input_ids[pos] = word2idx['[MASK]']\n",
                "            elif random_value > 0.9:\n",
                "                # 随机选择其他token 进行替换\n",
                "                index = randint(0, vocab_size -1)\n",
                "                while index < 3 or index == input_ids[pos]:\n",
                "                    index = randint(0, vocab_size -1)\n",
                "                input_ids[pos] = index\n",
                "            \n",
                "        # zero padding 的token 也要mask 15% （这一步很奇怪）\n",
                "        if max_pred > n_pred:\n",
                "            n_pad = max_pred - n_pred\n",
                "            masked_token = masked_token + [0] * n_pad\n",
                "            masked_pos = masked_pos + [0] * n_pad\n",
                "\n",
                "        # zero-padding\n",
                "        n_pads = max_seq_len - len(input_ids)\n",
                "        input_ids = input_ids + n_pads * [0]\n",
                "        segment_ids = segment_ids + n_pads * [0]\n",
                "        \n",
                "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
                "            batch.append([input_ids, segment_ids, masked_token, masked_pos, True])\n",
                "            positive += 1\n",
                "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
                "            batch.append([input_ids, segment_ids, masked_token, masked_pos, False])\n",
                "            negative += 1\n",
                "    return batch\n",
                "\n",
                "batch = make_data()\n",
                "print(len(batch))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "6\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "input_ids, segment_ids, masked_token, masked_pos, isNext = zip(*batch) # 此时 input_ids 是 list 类型，直接传入BERT model 会报错\n",
                "\n",
                "# 这一步不能少，否则在迭代数据时出问题\n",
                "input_ids, segment_ids, masked_token, masked_pos, isNext = \\\n",
                "    torch.LongTensor(input_ids), torch.LongTensor(segment_ids), torch.LongTensor(masked_token), torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
                "\n",
                "class MyDataSet(Data.Dataset):\n",
                "\n",
                "    def __init__(self, input_ids, segment_ids, masked_token, maksed_pos, isNext):\n",
                "        self.input_ids = input_ids\n",
                "        self.segment_ids = segment_ids\n",
                "        self.masked_token = masked_token\n",
                "        self.masked_pos = maksed_pos\n",
                "        self.isNext = isNext\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.input_ids)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return self.input_ids[idx], self.segment_ids[idx], self.masked_token[idx], self.masked_pos[idx], self.isNext[idx]\n",
                "\n",
                "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_token, masked_pos, isNext), batch_size, True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 训练 & 测试"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "from bert_util import BERT\n",
                "\n",
                "model = BERT(vocab_size=vocab_size, max_seq_len=max_seq_len, model_dim=model_dim,\n",
                "             heads_num=heads_num, ffn_dim=ffn_dim, layers_num=layers_num, dropout=dropout)\n",
                "\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adadelta(model.parameters(), lr=1e-3)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "for epoch in range(epoch_num):\n",
                "    for input_ids, segment_ids, masked_token, masked_pos, isNext in loader:\n",
                "        logits_clsf, logits_lm = model(input_ids, segment_ids, masked_pos)\n",
                "        loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_token.view(-1))\n",
                "        loss_lm = loss_lm.mean()\n",
                "        loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
                "        loss = loss_lm + loss_clsf\n",
                "        if (epoch + 1) % 10 == 0:  # 每迭代10epoch就打印一次loss\n",
                "            print(f'Epoch:{epoch + 1},  loss= {loss:.2f}')\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch:10,  loss= 3.83\n",
                        "Epoch:20,  loss= 3.43\n",
                        "Epoch:30,  loss= 3.10\n",
                        "Epoch:40,  loss= 2.86\n",
                        "Epoch:50,  loss= 2.69\n",
                        "Epoch:60,  loss= 2.56\n",
                        "Epoch:70,  loss= 2.47\n",
                        "Epoch:80,  loss= 2.40\n",
                        "Epoch:90,  loss= 2.33\n",
                        "Epoch:100,  loss= 2.28\n",
                        "Epoch:110,  loss= 2.22\n",
                        "Epoch:120,  loss= 2.18\n",
                        "Epoch:130,  loss= 2.13\n",
                        "Epoch:140,  loss= 2.09\n",
                        "Epoch:150,  loss= 2.05\n",
                        "Epoch:160,  loss= 2.01\n",
                        "Epoch:170,  loss= 1.97\n",
                        "Epoch:180,  loss= 1.93\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 测试一下"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "# Predict mask tokens and isNext\n",
                "model.eval()\n",
                "inputs_id, segment_ids, masked_token, masked_pos, isNext = batch[0]\n",
                "print(text)\n",
                "print('*****' * 10)\n",
                "print([idx2word[w] for w in inputs_id if idx2word[w] != '[PAD]'])\n",
                "print('*****'* 20)\n",
                "logits_clsf, logits_lm = model(torch.LongTensor([inputs_id]), \\\n",
                "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Hello, how are you? I am Romeo.\n",
                        "Hello, Romeo My name is Juliet. Nice to meet you.\n",
                        "Nice meet you too. How are you today?\n",
                        "Great. My baseball team won the competition.\n",
                        "Oh Congratulations, Juliet\n",
                        "Thank you Romeo\n",
                        "Where are you going today?\n",
                        "I am going shopping. What about you?\n",
                        "I am going to visit my grandmother. she is not very well\n",
                        "**************************************************\n",
                        "['[CLS]', 'great', 'my', 'baseball', 'team', 'won', 'the', 'competition', '[SEP]', 'great', 'my', 'baseball', '[MASK]', 'won', '[MASK]', 'competition', '[SEP]']\n",
                        "****************************************************************************************************\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "print(logits_lm.shape)\n",
                "print(logits_clsf.shape)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "torch.Size([1, 5, 40])\n",
                        "torch.Size([1, 2])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "clsf_score = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
                "if clsf_score:\n",
                "    print('isNext')\n",
                "else:\n",
                "    print('not isNext')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "not isNext\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "source": [
                "lm_indices = logits_lm.data.max(-1)[1][0].data.numpy()\n",
                "label_pos = [pos for pos in masked_token if pos !=0]\n",
                "predict_pos = [pos for pos in lm_indices if pos !=0]\n",
                "print('label pos', label_pos)\n",
                "print('predict pos:', predict_pos)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "label pos [16, 9]\n",
                        "predict pos: [16, 16]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.7.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7.3 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "56e331e9a2ee1d59a709fc58c9503a214b87f78fd6d7d2d4ac9f44a5699c674f"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}